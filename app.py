import streamlit as st
import cv2
import numpy as np
import tensorflow as tf
from tensorflow.keras.models import load_model
import mediapipe as mp
import os
from collections import deque

st.set_page_config(
    page_title="Sign Language Recognition",
    page_icon="âœ‹",
    layout="wide",
    initial_sidebar_state="expanded"
)

st.markdown("""
<style>
    .main-header { font-size: 3rem; color: #1f77b4; text-align: center; margin-bottom: 2rem; }
    .prediction-box { background-color: #1f2124; padding: 2rem; border-radius: 10px; text-align: center; margin: 1rem 0; }
    .confidence-bar { height: 20px; background-color: #1f77b4; border-radius: 10px; margin: 0.5rem 0; }
    .sentence-box { background-color: #e6f7ff; padding: 1.5rem; border-radius: 10px; border-left: 5px solid #1f77b4; 
                   margin: 1rem 0; font-size: 1.5rem; min-height: 80px; }
    .success-box { background-color: #30cf56; padding: 1rem; border-radius: 10px; border-left: 5px solid #28a745; }
</style>
""", unsafe_allow_html=True)

st.markdown('<h1 class="main-header">âœ‹ Perfect Sign Language Recognition</h1>', unsafe_allow_html=True)
st.markdown('<div class="success-box"><strong>âœ… Model Accuracy: 100%</strong> - Ready for real-time recognition!</div>', unsafe_allow_html=True)

actions = np.array(['hello', 'iloveyou', 'thanks'])


if 'model' not in st.session_state:
    st.session_state.model = None
if 'mean' not in st.session_state:
    st.session_state.mean = None
if 'std' not in st.session_state:
    st.session_state.std = None
if 'is_recording' not in st.session_state:
    st.session_state.is_recording = False
if 'sequence' not in st.session_state:
    st.session_state.sequence = []
if 'predictions' not in st.session_state:
    st.session_state.predictions = deque(maxlen=5)
if 'current_sentence' not in st.session_state:
    st.session_state.current_sentence = []

@st.cache_resource
def load_assets():
    try:
        if os.path.exists('best_model.keras'):
            model = load_model('best_model.keras')
        else:
            model = load_model('best_model.h5')
        
        mean = np.load('x_mean.npy')
        std = np.load('x_std.npy')
        return model, mean, std
    except Exception as e:
        st.error(f"Error loading files: {e}")
        return None, None, None

mp_holistic = mp.solutions.holistic
mp_drawing = mp.solutions.drawing_utils

def extract_keypoints(results):
    """Extract hand landmarks from MediaPipe results"""
    rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21*3)
    lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21*3)
    return np.concatenate([rh, lh])

def mediapipe_detection(image, model):
    """Process image with MediaPipe"""
    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
    image.flags.writeable = False
    results = model.process(image)
    image.flags.writeable = True
    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)
    return image, results


with st.sidebar:
    st.header("âš™ï¸ Settings")
    
    if st.button("ðŸš€ Load Perfect Model", width='stretch', type="primary"):
        with st.spinner("Loading 100% accurate model..."):
            st.session_state.model, st.session_state.mean, st.session_state.std = load_assets()
            if st.session_state.model:
                st.success(" Perfect model loaded!")
            else:
                st.error("Failed to load model")
    
    st.header("ðŸŽ® Controls")
    if st.button("ðŸŽ¥ Start/Stop Recording", width='stretch'):
        st.session_state.is_recording = not st.session_state.is_recording
        st.session_state.sequence = []
        st.session_state.predictions = deque(maxlen=5)
    
    status_text = "ðŸ”´ Recording" if st.session_state.is_recording else "âš« Stopped"
    st.write(f"**Status:** {status_text}")
    
    st.header("ðŸ“ Sentence Builder")
    
    if st.button("âž• Add to Sentence", width='stretch'):
        if st.session_state.predictions:
            recent_pred = max(set(st.session_state.predictions), key=list(st.session_state.predictions).count)
            st.session_state.current_sentence.append(actions[recent_pred])
    
    if st.button("ðŸ—‘ï¸ Clear Sentence", width='stretch'):
        st.session_state.current_sentence = []
    
    confidence_threshold = st.slider("ðŸŽ¯ Confidence Threshold", 0.0, 1.0, 0.7, 0.05)

col1, col2 = st.columns([2, 1])

with col1:
    st.header("ðŸ“¹ Live Webcam Feed")
    webcam_placeholder = st.empty()

with col2:
    st.header("ðŸŽ¯ Current Prediction")
    prediction_placeholder = st.empty()
    st.header("ðŸ“ Built Sentence")
    sentence_placeholder = st.empty()

cap = cv2.VideoCapture(0)

if st.session_state.model and st.session_state.mean is not None and st.session_state.std is not None:
    with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:
        while cap.isOpened():
            ret, frame = cap.read()
            if not ret:
                st.error("Failed to access webcam")
                break

            image, results = mediapipe_detection(frame, holistic)
            
            if results.right_hand_landmarks:
                mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS)
            if results.left_hand_landmarks:
                mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS)
            
            keypoints = extract_keypoints(results)
            
            if st.session_state.is_recording:
                st.session_state.sequence.append(keypoints)
                st.session_state.sequence = st.session_state.sequence[-30:]
                
                if len(st.session_state.sequence) == 30:
                    input_data = (np.array(st.session_state.sequence) - st.session_state.mean) / st.session_state.std
                    input_data = np.expand_dims(input_data, axis=0)
                    
                    res = st.session_state.model.predict(input_data, verbose=0)[0]
                    predicted_class = np.argmax(res)
                    confidence = res[predicted_class]
                    
                    if confidence > confidence_threshold:
                        st.session_state.predictions.append(predicted_class)
                    
                    if st.session_state.predictions:
                        current_prediction = max(set(st.session_state.predictions), key=list(st.session_state.predictions).count)
                        
                        with prediction_placeholder:
                            st.markdown(f"""
                            <div class="prediction-box">
                                <h2>{actions[current_prediction].upper()}</h2>
                                <p>Confidence: {confidence:.2%}</p>
                                <div style="width: {confidence*100}%;" class="confidence-bar"></div>
                            </div>
                            """, unsafe_allow_html=True)
            
            with sentence_placeholder:
                if st.session_state.current_sentence:
                    sentence_text = " ".join(st.session_state.current_sentence)
                    st.markdown(f"""
                    <div class="sentence-box">
                        <strong>{sentence_text}</strong>
                    </div>
                    """, unsafe_allow_html=True)
                else:
                    st.info("Your sentence will appear here. Use 'Add to Sentence' to add words.")
            
            image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
            webcam_placeholder.image(image_rgb, channels="RGB", width='stretch')
            
            if cv2.waitKey(10) & 0xFF == ord('q'):
                break
    
    cap.release()
else:
    st.warning("Please load the model from the sidebar to start recognition.")

st.markdown("---")
st.markdown("**âœ¨ Perfect Sign Language Recognition - 100% Accurate Model**")
